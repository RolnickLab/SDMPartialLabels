{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2c51f2",
   "metadata": {},
   "source": [
    "This notebook prepares the ebutterfly data, not co-located with ebird), data from USA (starting year 2010), we  start with clustering the observations, creates polygons to extract the satellite images from planetary computer, filters images that are smaller than 128x128, creates the targets by aggregating the checklists, saves final csv for the hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21decd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path    \n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from geopy.distance import great_circle\n",
    "from shapely.geometry import MultiPoint\n",
    "from shapely.geometry import Polygon, Point\n",
    "from math import cos, radians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188a0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home/hagerradi/projects/Ecosystem_embeddings/ebutterfly/Darwin/0177350-230224095556074\"\n",
    "dataset_tag = \"SatButterfly_v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ca6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter locations outside the continental US\n",
    "def filter_locations(df): # (slow) takes around 30 mins\n",
    "    gdf = gpd.read_file(os.path.join(root_dir, \"cb_2018_us_nation_5m/cb_2018_us_nation_5m.shp\"))\n",
    "    indices = []\n",
    "    for i, row in tqdm(df.iterrows()):\n",
    "        if not gdf.geometry[0].contains(Point(row[\"decimalLongitude\"], row[\"decimalLatitude\"])):\n",
    "            indices += [i]\n",
    "\n",
    "    df_clean = df.drop(indices)\n",
    "    return df_clean\n",
    "\n",
    "def get_main_df(df_file_name):\n",
    "    buttefly_data_US = pd.read_csv(os.path.join(root_dir, df_file_name))\n",
    "\n",
    "    # exclude observations in states\n",
    "    states_to_exclude = ['Alaska', 'Hawaii']\n",
    "    buttefly_data_US = buttefly_data_US[~buttefly_data_US['stateProvince'].isin(states_to_exclude)]\n",
    "    \n",
    "    # filter out based on USA geography\n",
    "    buttefly_data_US = filter_locations(buttefly_data_US)\n",
    "\n",
    "    return buttefly_data_US\n",
    "\n",
    "butterfly_data_US = get_main_df(\"occ_usa.csv\")\n",
    "print(butterfly_data_US)\n",
    "butterfly_data_US.to_csv(os.path.join(root_dir, \"occ_usa_geo_filtered.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04ba6c1",
   "metadata": {},
   "source": [
    "- occ_usa.csv is extracted from ebutterfly raw observations (ebutterfly/Darwin/0177350-230224095556074/occurrence.txt, where country code in the US and starting the year 2010\n",
    "- If the folder cb_2018_us_nation_5m does not exist, you can download from: https://www.census.gov/geographies/mapping-files/2018/geo/carto-boundary-file.html "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe35ee7",
   "metadata": {},
   "source": [
    "# Clustering ebutterfly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e8d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_ebutterfly_data(df):\n",
    "    RADIUS_EARTH = 6356.7523 \n",
    "\n",
    "    coordinates = df[['decimalLatitude', 'decimalLongitude']].values\n",
    "\n",
    "    eps = 1/RADIUS_EARTH # Maximum distance between points to be considered part of the same cluster\n",
    "    min_samples = 2  # Minimum number of points in a cluster (including the core point)\n",
    "\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples, algorithm='ball_tree', metric='haversine').fit(np.radians(coordinates))\n",
    "\n",
    "    cluster_labels = db.labels_\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise (-1 is noise)\n",
    "    num_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "    num_noise = len(set(cluster_labels)) - num_clusters\n",
    "    print(\"Number of clusters:\", num_clusters)\n",
    "    print(\"Number of noise:\", num_noise)\n",
    "\n",
    "    clusters = pd.Series([coordinates[cluster_labels == n] for n in range(num_clusters)])\n",
    "\n",
    "    # print(clusters)\n",
    "    def get_centermost_point(cluster):\n",
    "        centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y)\n",
    "        centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m)\n",
    "        return tuple(centermost_point)\n",
    "\n",
    "    centermost_points = clusters.map(get_centermost_point)\n",
    "    center_lats, center_lons = zip(*centermost_points)\n",
    "\n",
    "    # save final dataframe\n",
    "    df[\"cluster_label\"] = cluster_labels\n",
    "\n",
    "    df = df[df[\"cluster_label\"] != -1]\n",
    "    print(df)\n",
    "    cluster_labels = cluster_labels[np.where(cluster_labels != -1)]\n",
    "\n",
    "    df[\"center_lat\"] = [center_lats[cl] for cl in cluster_labels]\n",
    "    df[\"center_lon\"] = [center_lons[cl] for cl in cluster_labels]\n",
    "    df[\"hotspot_id\"] = [\"L\" + str(cl) for cl in cluster_labels]\n",
    "\n",
    "    df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "butterfly_data_US = pd.read_csv(os.path.join(root_dir, \"occ_usa_geo_filtered.csv\"))\n",
    "butterfly_data_US_clustered = cluster_ebutterfly_data(df=butterfly_data_US)\n",
    "butterfly_data_US_clustered.to_csv(os.path.join(root_dir, dataset_tag, \"butterfly_data_clustered.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00809efc",
   "metadata": {},
   "source": [
    "# Generate satellite images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2aa24e",
   "metadata": {},
   "source": [
    "### 1. Create polygons for the lats, lons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b061e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_buffer_meter(data, radius, geometry='geometry', crs='epsg:4326', projected_crs='epsg:3857'): \n",
    "    \"\"\" Generates a buffer around the geometries in a geopandas DataFrame. \n",
    "    Parameters: \n",
    "        data (GeoDataFrame or DataFrame): The geopandas dataframe or a pandas dataframe that contains geometry data. \n",
    "        radius (float): The radius of the buffer in meters. \n",
    "        geometry (str, optional): The column in the dataframe that contains the geometry information. Defaults to 'geometry'. \n",
    "        crs (str, optional): The Coordinate Reference System of the input geometries. Defaults to 'epsg:4326'. \n",
    "        projected_crs (str, optional): The projected CRS to use for buffering. Defaults to 'epsg:3857'. \n",
    "    Returns: \n",
    "        GeoDataFrame: A new geopandas dataframe with the buffer applied to the geometry. \n",
    "    \"\"\" \n",
    "    data = gpd.GeoDataFrame(data) \n",
    "    data = data.to_crs(projected_crs)\n",
    "    data[geometry] = data[geometry].buffer(radius, cap_style=3)\n",
    "    data = data.to_crs(crs)\n",
    "    return data\n",
    "\n",
    "def generate_geometry(df):\n",
    "    df = df.drop_duplicates().reset_index()\n",
    "\n",
    "    geometry = [Point(xy) for xy in zip(df['center_lon'], df['center_lat'])]\n",
    "    crs = {'init':'epsg:4326'}\n",
    "\n",
    "    geo_df = gpd.GeoDataFrame(df, crs=crs, geometry=geometry)\n",
    "\n",
    "    data_df = generate_buffer_meter(geo_df, 2500)\n",
    "    return data_df\n",
    "\n",
    "\n",
    "center_data_df = pd.read_csv(os.path.join(root_dir, dataset_tag, \"butterfly_data_clustered.csv\"), usecols=[\"hotspot_id\", \"center_lon\", \"center_lat\"])\n",
    "data_df = generate_geometry(center_data_df)\n",
    "\n",
    "print(data_df.shape)\n",
    "\n",
    "print(data_df)\n",
    "\n",
    "data_df.to_csv(os.path.join(root_dir, dataset_tag, \"ebutterfly_center_polygons.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e226b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "def plot_coordinates(df):\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    colors = {'train': 'b', 'test':'y', 'valid':'m'}\n",
    "\n",
    "    ax.scatter(x=df['center_lon'], y=df['center_lat'], color='red')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    ax.set_title('Coordinates on USA Map')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_coordinates(butterfly_data_US_clustered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03af1204",
   "metadata": {},
   "source": [
    "### 2. use the polygons file to extract satellite images from planetary compute, using the script (data_processing/ebutterfly_data_preparation/download_rasters_from_planetary_computer.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c6e29c",
   "metadata": {},
   "source": [
    "### 3. Filter satellite images and save final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5255161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import shutil\n",
    "from pathlib import Path    \n",
    "import os\n",
    "\n",
    "def filter_satellite_images(dst, image_folder=\"raw_images\"):\n",
    "    # exclude images less than 128x128\n",
    "    if not os.path.exists(dst):\n",
    "        os.makedirs(dst)\n",
    "    \n",
    "    # src\n",
    "    file_list = glob.glob(os.path.join(root_dir, dataset_tag, image_folder,  \"*\"))\n",
    "    for i, file_path in enumerate(file_list):\n",
    "        with rio.open(file_path) as f:\n",
    "            r = f.read(3)\n",
    "            g = f.read(2)\n",
    "            b = f.read(1)\n",
    "        composite = np.stack((r, g, b), axis=-1)\n",
    "        if composite.shape[0] >= 128 and composite.shape[1] >= 128:\n",
    "            shutil.copy(file_path, dst)\n",
    "\n",
    "def get_final_hotspots(dst):\n",
    "    final_hotspots = []\n",
    "    file_list = glob.glob(os.path.join(dst, \"*\"))\n",
    "    for i, file_path in enumerate(file_list):\n",
    "        final_hotspots.append(str(Path(file_path).name.split(\".\")[0][1:]))\n",
    "    \n",
    "    print(len(final_hotspots))\n",
    "    \n",
    "    return final_hotspots\n",
    "\n",
    "def explore_satellite_images(image_folder):\n",
    "    file_list = glob.glob(os.path.join(root_dir, dataset_tag, image_folder, \"*\"))\n",
    "\n",
    "    # Select 8 random files from the list\n",
    "    random_files = random.sample(file_list, 8)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "\n",
    "    for i, file_path in enumerate(random_files):\n",
    "        with rio.open(file_path) as f:\n",
    "            r = f.read(3)\n",
    "            g = f.read(2)\n",
    "            b = f.read(1)\n",
    "\n",
    "        # Create a composite image from RGB channels\n",
    "        composite = np.stack((r, g, b), axis=-1)\n",
    "        print(composite.shape)\n",
    "\n",
    "        # Clip and normalize the values\n",
    "        normalized_composite = np.clip((composite / 10000), 0, 1)\n",
    "\n",
    "        # Get the title from the file name\n",
    "        title = file_path.split(\"/\")[-1]\n",
    "\n",
    "        # Plot the image in the corresponding subplot\n",
    "        ax = axes[i // 4, i % 4]\n",
    "        ax.imshow(normalized_composite)\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "    # Adjust spacing and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "explore_satellite_images(image_folder=\"images\")\n",
    "# exclude images of smaller sizes\n",
    "dst = os.path.join(root_dir, dataset_tag, \"images\")\n",
    "# filter_satellite_images(dst=dst, image_folder=\"raw_images\")\n",
    "final_hotspots = get_final_hotspots(dst)\n",
    "print(final_hotspots)\n",
    "\n",
    "# butterfly_df = pd.read_csv(os.path.join(root_dir, dataset_tag, \"butterfly_hotspots.csv\"))\n",
    "\n",
    "# butterfly_df = butterfly_df[butterfly_df['hotspot_id'].isin(final_hotspots)]\n",
    "\n",
    "# butterfly_df.to_csv(os.path.join(root_dir, dataset_tag, \"butterfly_hotspots_final.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99b8d43",
   "metadata": {},
   "source": [
    "# Create hotspots csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a798073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group data of each cluster\n",
    "def group_clusters(butterfly_df):\n",
    "    grouped_butterfly_data = butterfly_df.groupby(['hotspot_id'])\n",
    "        \n",
    "    group_sizes = grouped_butterfly_data.size()\n",
    "    \n",
    "    print(group_sizes)\n",
    "    \n",
    "    return grouped_butterfly_data\n",
    "\n",
    "butterfly_df = pd.read_csv(os.path.join(root_dir, dataset_tag, \"butterfly_data_clustered.csv\"))\n",
    "# exclude the clusters that have no satellite images\n",
    "butterfly_df = butterfly_df[butterfly_df['hotspot_id'].isin(final_hotspots)]\n",
    "\n",
    "grouped_butterfly_data = group_clusters(butterfly_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a93a8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save species list of all unique species\n",
    "def save_species_list(df):\n",
    "    species_list = df[\"species\"].unique().tolist()\n",
    "    species_df = df['species'].value_counts()\n",
    "    \n",
    "    species_df = species_df.reset_index()\n",
    "    species_df.columns = ['species', 'frequency']\n",
    "    \n",
    "    species_df.to_csv(os.path.join(root_dir, dataset_tag, 'species_list.csv'), index=False)\n",
    "    \n",
    "    return species_df\n",
    "\n",
    "butterfly_df = pd.read_csv(os.path.join(root_dir, dataset_tag, \"butterfly_data_clustered.csv\"))\n",
    "# exclude the clusters that have no satellite images\n",
    "butterfly_df = butterfly_df[butterfly_df['hotspot_id'].isin(final_hotspots)]\n",
    "\n",
    "species_list = save_species_list(butterfly_df)\n",
    "# print(species_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b12572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an intermediate step done once: \n",
    "# saving the list of species (common and union) between the hotspots here and ebird hotspots\n",
    "species_list = pd.read_csv(os.path.join(root_dir, dataset_tag, 'species_list.csv'))\n",
    "species_list_2 = pd.read_csv(os.path.join(root_dir, \"SatButterfly_v2\", 'species_list.csv'))\n",
    "\n",
    "print(species_list)\n",
    "print(species_list_2)\n",
    "species_list = species_list[species_list[\"frequency\"] >= 100]\n",
    "species_list_2 = species_list_2[species_list_2[\"frequency\"] >= 100]\n",
    "\n",
    "print(species_list)\n",
    "print(species_list_2)\n",
    "\n",
    "# species_list.to_csv(os.path.join(root_dir, dataset_tag, 'species_list_>=100.csv'), index=False)\n",
    "# species_list_2.to_csv(os.path.join(root_dir, \"SatButterfly_v2\", 'species_list_>=100.csv'), index=False)\n",
    "\n",
    "final_species_list_intersect = np.intersect1d(species_list['species'].values.tolist(), species_list_2['species'].values.tolist())\n",
    "final_species_list_union = np.union1d(species_list['species'].values.tolist(), species_list_2['species'].values.tolist())\n",
    "\n",
    "print(final_species_list_intersect)\n",
    "\n",
    "# with open(os.path.join(root_dir, \"intersection_species_list_>=100.txt\"), 'w') as outfile:\n",
    "#     outfile.write('\\n'.join(str(species_name) for species_name in final_species_list_intersect))\n",
    "\n",
    "# with open(os.path.join(root_dir, \"union_species_list_>=100.txt\"), 'w') as outfile:\n",
    "#     outfile.write('\\n'.join(str(species_name) for species_name in final_species_list_union))\n",
    "\n",
    "# save new species lists with the intersection of species only\n",
    "new_species_list_2 = species_list_2[species_list_2['species'].isin(final_species_list_intersect)]\n",
    "# new_species_list_2.to_csv(os.path.join(root_dir, \"SatButterfly_v2\", 'species_list_updated.csv'), index=False)\n",
    "\n",
    "new_species_list = species_list[species_list['species'].isin(final_species_list_intersect)]\n",
    "print(new_species_list)\n",
    "print(new_species_list_2)\n",
    "# new_species_list.to_csv(os.path.join(root_dir, dataset_tag, 'species_list_updated.csv'), index=False)\n",
    "\n",
    "\n",
    "# species_list = new_species_list['species'].values.tolist()\n",
    "# print(len(species_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ddcc7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create targets by aggregating checklists\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_target_files(grouped_butterfly_data, output_folder, species_list):\n",
    "    for group_name, group_data in tqdm(grouped_butterfly_data):\n",
    "#         print(group_data['decimalLatitude'], group_data['decimalLongitude'])\n",
    "        target = {}\n",
    "        checklist_ = np.zeros(len(species_list))\n",
    "        # dropping species that appear more than once in the same checklist\n",
    "        new_df = group_data[['eventID', 'species']]\n",
    "        new_df = new_df.drop_duplicates(['eventID', 'species'])\n",
    "\n",
    "        for sp in new_df['species']:\n",
    "            if sp in species_list:\n",
    "                checklist_[species_list.index(sp)] += 1\n",
    "        target['num_complete_checklists'] = len(group_data['eventID'].unique())\n",
    "        checklist_ = checklist_ / target['num_complete_checklists']\n",
    "        if np.max(checklist_) > 1 :\n",
    "            problamtic.append(group_name)\n",
    "\n",
    "        target['probs'] = checklist_.tolist()\n",
    "        target['hotspot_id'] = group_name\n",
    "    \n",
    "        with open(os.path.join(output_folder, \"B\"+str(group_name) + \".json\"), 'w') as fp:\n",
    "            json.dump(target, fp)\n",
    "\n",
    "dst = os.path.join(root_dir, dataset_tag, \"butterfly_targets_v1.2\")\n",
    "if not os.path.exists(dst):\n",
    "    os.makedirs(dst)\n",
    "\n",
    "print(final_species_list_intersect)\n",
    "compute_target_files(grouped_butterfly_data, dst, final_species_list_intersect.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bcdcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final csv\n",
    "# columns: hotspot_name, lon, lat, number_of_observations, number_of_unique_checklists, number_of_unique_species, env variables\n",
    "def save_final_csv(grouped_butterfly_data, output_filename='butterfly_hotspots.csv'):\n",
    "    hotspot_ids = []\n",
    "    lats, lons = [], []\n",
    "    number_of_butterfly_obs = []\n",
    "    number_of_unique_checklists = []\n",
    "    number_of_different_species = []\n",
    "    states = []\n",
    "\n",
    "    for group_name, group_data in tqdm(grouped_butterfly_data):\n",
    "        if group_name in final_hotspots:\n",
    "            hotspot_ids.append(str(group_name))\n",
    "            lats.append(group_data['center_lat'].iloc[0])\n",
    "            lons.append(group_data['center_lon'].iloc[0])\n",
    "            states.append(group_data['stateProvince'].iloc[0])\n",
    "            number_of_butterfly_obs.append(len(group_data['occurrenceID']))\n",
    "            number_of_unique_checklists.append(len(group_data['eventID'].unique()))\n",
    "            number_of_different_species.append(len(group_data['species'].unique()))\n",
    "\n",
    "    final_data_frame = pd.DataFrame({'hotspot_id': hotspot_ids,\n",
    "                                     'lat': lats,\n",
    "                                     'lon': lons,\n",
    "                                     'stateProvince': states,\n",
    "                                     'ebutterfly_occurances': number_of_butterfly_obs,\n",
    "                                     'num_checklists': number_of_unique_checklists,\n",
    "                                     'num_species': number_of_different_species})\n",
    "\n",
    "    print(final_data_frame)\n",
    "\n",
    "    final_data_frame.to_csv(os.path.join(root_dir, dataset_tag, output_filename) , index=False)\n",
    "    \n",
    "\n",
    "save_final_csv(grouped_butterfly_data, 'butterfly_hotspots.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f7863b",
   "metadata": {},
   "source": [
    "## Extract environmental rasters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380b6cf0",
   "metadata": {},
   "source": [
    "#### use the polygons file to extract environmental rasters, using the script (data_processing/environmental/get_env_var.py)\n",
    "#### preferably on the cluster for the use of (geolifeclef-2022/rasters) data, using 'butterfly_hotspots.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e5d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = os.path.join(root_dir, dataset_tag, \"environmental_data\")\n",
    "if not os.path.exists(dst):\n",
    "    os.makedirs(dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a1840b",
   "metadata": {},
   "source": [
    "### split data using DBSCAN (script: data_processing/utils/make_splits_by_distance.py), using the final csv (butterfly_hotspots.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b3d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_different_split_files(butterfly_data_with_split):\n",
    "    grouped_butterfly_data = butterfly_data_with_split.groupby(['split'], as_index=False)\n",
    "\n",
    "    for group_name, group_data in tqdm(grouped_butterfly_data):\n",
    "        print(group_name)\n",
    "        print(group_data[\"ebutterfly_occurances\"].max())\n",
    "        group_data.to_csv(os.path.join(root_dir, dataset_tag, \"butterfly_hotspots_\" + str(group_name) + \".csv\"))\n",
    "        \n",
    "butterfly_data_with_split = pd.read_csv(os.path.join(root_dir, dataset_tag, \"butterfly_hotspots_with_splits.csv\"))\n",
    "\n",
    "save_different_split_files(butterfly_data_with_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e070619c",
   "metadata": {},
   "source": [
    "### Visualize map after splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6afe36f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot final splits\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "sys.path.append(str(Path().resolve().parent.parent))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "def plot_final_splits(df):\n",
    "    geoDatav = gpd.read_file('https://raw.githubusercontent.com/holtzy/The-Python-Graph-Gallery/master/static/data/US-counties.geojson')\n",
    "\n",
    "    geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry)   \n",
    "\n",
    "    ig, ax = plt.subplots(figsize =(15,10))\n",
    "    #train_gdf.drop_duplicates([\"geometry\"]).boundary.plot(ax = ax, alpha = 0.4, edgecolor = \"gray\")\n",
    "    geoDatav[~geoDatav[\"STATE\"].isin([\"02\", \"15\"])].boundary.plot(ax=ax, alpha = 0.1, edgecolor = \"gray\" )\n",
    "    gdf[gdf[\"split\"]==\"train\"].plot(ax=ax,marker='o', color='mediumslateblue', markersize=1, label = \"train\")\n",
    "    gdf[gdf[\"split\"]==\"val\"].plot(ax=ax, marker='o', color='lightseagreen', markersize=1, label = \"val\")\n",
    "    gdf[gdf[\"split\"]==\"test\"].plot(ax=ax, marker='o', color='lightsalmon', markersize=1, label = \"test\")\n",
    "\n",
    "    plt.legend(fontsize=16, markerscale=5,loc='lower right',  bbox_to_anchor=(0.92, 0.25))\n",
    "    plt.title(\"butterfly Hotspots\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "path = os.path.join(root_dir, dataset_tag, \"butterfly_hotspots_with_splits.csv\")\n",
    "plot_final_splits(df=pd.read_csv(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98ab058",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(root_dir, \"ebutterfly_data_v2\", \"butterfly_hotspots_with_splits.csv\")\n",
    "plot_final_splits(pd.read_csv(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab83f14",
   "metadata": {},
   "source": [
    "Final files saved:\n",
    "[('valid', 1147), ('test', 1145), ('train', 5316)]\n",
    "- butterfly_hotspots.csv\n",
    "- butterfly_hotspots_train.csv\n",
    "- butterfly_hotspots_valid.csv\n",
    "- butterfly_hotspots_test.csv\n",
    "- species_list.csv\n",
    "- butterfly_targets/\n",
    "- environmental_data/\n",
    "- images\n",
    "- images_visual"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
