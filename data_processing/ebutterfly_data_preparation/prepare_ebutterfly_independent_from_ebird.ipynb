{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook prepares the ebutterfly data, not co-located with ebird), titled \"SatButterfly-1\", data from USA (starting year 2010), we  start with clustering the observations, creates polygons to extract the satellite images from planetary computer, filters images that are smaller than 128x128, creates the targets by aggregating the checklists, saves final csv for the hotspots"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17174c07580b78d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path    \n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from geopy.distance import great_circle\n",
    "from shapely.geometry import MultiPoint\n",
    "from shapely.geometry import Polygon, Point\n",
    "from math import cos, radians\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio as rio\n",
    "import shutil\n",
    "from pathlib import Path    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c350abb8f8422d5d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "root_dir = \"SatButterfly_dataset\"\n",
    "dataset_tag = \"SatButterfly_v1\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eafed9059d44cbce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def filter_locations(df): # (slow) takes around 30 mins\n",
    "    \"\"\"\n",
    "    filter locations outside the continental US using the boundaries shape file cb_2018_us_nation_5m\n",
    "    Parameters:\n",
    "        df: Pandas dataframe with all observations\n",
    "    Returns:\n",
    "        df_clean: Pandas dataframe with filtered observations\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(os.path.join(root_dir, \"cb_2018_us_nation_5m/cb_2018_us_nation_5m.shp\"))\n",
    "    indices = []\n",
    "    for i, row in tqdm(df.iterrows()):\n",
    "        if not gdf.geometry[0].contains(Point(row[\"decimalLongitude\"], row[\"decimalLatitude\"])):\n",
    "            indices += [i]\n",
    "\n",
    "    df_clean = df.drop(indices)\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def get_main_df(df_file_name):\n",
    "    \"\"\"\n",
    "    return dataframe after location filtering\n",
    "    Parameters:\n",
    "        df_filename: filename of dataframe\n",
    "    Returns:\n",
    "        Pandas dataframe\n",
    "    \"\"\"\n",
    "    butterfly_data_US = pd.read_csv(os.path.join(root_dir, df_file_name))\n",
    "    # exclude observations in states\n",
    "    states_to_exclude = ['Alaska', 'Hawaii']\n",
    "    butterfly_data_US = butterfly_data_US[~butterfly_data_US['stateProvince'].isin(states_to_exclude)]\n",
    "    \n",
    "    # filter out based on USA geography\n",
    "    butterfly_data_US = filter_locations(butterfly_data_US)\n",
    "\n",
    "    return butterfly_data_US\n",
    "\n",
    "butterfly_data_US = get_main_df(\"occ_usa.csv\")\n",
    "print(butterfly_data_US.head())\n",
    "\n",
    "# saving intermediate files to avoid repeating steps when not necessary\n",
    "butterfly_data_US.to_csv(os.path.join(root_dir, \"occ_usa_geo_filtered.csv\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71c6ba4e359390b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- occ_usa.csv is extracted from ebutterfly raw observations (ebutterfly/Darwin/0177350-230224095556074/occurrence.txt, where country code in the US and starting the year 2010\n",
    "- If the folder cb_2018_us_nation_5m does not exist, you can download from: https://www.census.gov/geographies/mapping-files/2018/geo/carto-boundary-file.html "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4de8ebeab1dfdea5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clustering ebutterfly data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18b76d6dce40bc7a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cluster_ebutterfly_data(df):\n",
    "    \"\"\"\n",
    "    clusters ebutterfly observations using DBSCAN based on haversine distance \n",
    "    Parameters:\n",
    "        df: Pandas dataframe of ebutterfly observations\n",
    "    Returns:\n",
    "        df: Pandas dataframe with observations assigned to centriods\n",
    "    \"\"\"\n",
    "    RADIUS_EARTH = 6356.7523 \n",
    "\n",
    "    coordinates = df[['decimalLatitude', 'decimalLongitude']].values\n",
    "\n",
    "    eps = 1/RADIUS_EARTH # Maximum distance between points to be considered part of the same cluster\n",
    "    min_samples = 2  # Minimum number of points in a cluster (including the core point)\n",
    "\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples, algorithm='ball_tree', metric='haversine').fit(np.radians(coordinates))\n",
    "\n",
    "    cluster_labels = db.labels_\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise (-1 is noise)\n",
    "    num_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "    num_noise = len(set(cluster_labels)) - num_clusters\n",
    "    print(\"Number of clusters:\", num_clusters)\n",
    "    print(\"Number of noise:\", num_noise)\n",
    "\n",
    "    clusters = pd.Series([coordinates[cluster_labels == n] for n in range(num_clusters)])\n",
    "\n",
    "    def get_centermost_point(cluster):\n",
    "        centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y)\n",
    "        centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m)\n",
    "        return tuple(centermost_point)\n",
    "\n",
    "    centermost_points = clusters.map(get_centermost_point)\n",
    "    center_lats, center_lons = zip(*centermost_points)\n",
    "\n",
    "    # save final dataframe\n",
    "    df[\"cluster_label\"] = cluster_labels\n",
    "\n",
    "    df = df[df[\"cluster_label\"] != -1]\n",
    "\n",
    "    cluster_labels = cluster_labels[np.where(cluster_labels != -1)]\n",
    "\n",
    "    df[\"center_lat\"] = [center_lats[cl] for cl in cluster_labels]\n",
    "    df[\"center_lon\"] = [center_lons[cl] for cl in cluster_labels]\n",
    "    # assign hotspot ID to each cluster with prefix L\n",
    "    df[\"hotspot_id\"] = [\"L\" + str(cl) for cl in cluster_labels]\n",
    "\n",
    "    df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# read csv of filtered observations\n",
    "butterfly_data_US = pd.read_csv(os.path.join(root_dir, \"occ_usa_geo_filtered.csv\"))\n",
    "butterfly_data_US_clustered = cluster_ebutterfly_data(df=butterfly_data_US)\n",
    "\n",
    "# save clustered butterfly dataset\n",
    "butterfly_data_US_clustered.to_csv(os.path.join(root_dir, dataset_tag, \"butterfly_data_clustered.csv\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33c8aaefe02d74fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate satellite images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "654d70fcad8e7397"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Create polygons for the lats, lons"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31765bc4bb25e981"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_buffer_meter(data, radius, geometry='geometry', crs='epsg:4326', projected_crs='epsg:3857'): \n",
    "    \"\"\" Generates a buffer around the geometries in a geopandas DataFrame. \n",
    "    Parameters: \n",
    "        data (GeoDataFrame or DataFrame): The geopandas dataframe or a pandas dataframe that contains geometry data. \n",
    "        radius (float): The radius of the buffer in meters. \n",
    "        geometry (str, optional): The column in the dataframe that contains the geometry information. Defaults to 'geometry'. \n",
    "        crs (str, optional): The Coordinate Reference System of the input geometries. Defaults to 'epsg:4326'. \n",
    "        projected_crs (str, optional): The projected CRS to use for buffering. Defaults to 'epsg:3857'. \n",
    "    Returns: \n",
    "        data: GeoDataFrame: A new geopandas dataframe with the buffer applied to the geometry. \n",
    "    \"\"\" \n",
    "    data = gpd.GeoDataFrame(data) \n",
    "    data = data.to_crs(projected_crs)\n",
    "    data[geometry] = data[geometry].buffer(radius, cap_style=3)\n",
    "    data = data.to_crs(crs)\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_geometry(df):\n",
    "    \"\"\" generates geometry using with radius 2500 around center hotspots\n",
    "    Parameters: \n",
    "        df: Dataframe with ebutterfly clustered hotspots\n",
    "    Returns:\n",
    "        data_df: GeoDataFrame \n",
    "    \"\"\"\n",
    "    df = df.drop_duplicates().reset_index()\n",
    "\n",
    "    geometry = [Point(xy) for xy in zip(df['center_lon'], df['center_lat'])]\n",
    "    crs = {'init':'epsg:4326'}\n",
    "\n",
    "    geo_df = gpd.GeoDataFrame(df, crs=crs, geometry=geometry)\n",
    "\n",
    "    data_df = generate_buffer_meter(geo_df, 2500)\n",
    "    return data_df\n",
    "\n",
    "\n",
    "center_data_df = pd.read_csv(os.path.join(root_dir, dataset_tag, \"butterfly_data_clustered.csv\"), usecols=[\"hotspot_id\", \"center_lon\", \"center_lat\"])\n",
    "data_df = generate_geometry(center_data_df)\n",
    "\n",
    "print(data_df.shape)\n",
    "print(data_df.head())\n",
    "\n",
    "data_df.to_csv(os.path.join(root_dir, dataset_tag, \"butterfly_center_polygons.csv\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2327d40d83419947"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. use the polygons file to extract satellite images from planetary compute, using the script (data_processing/ebutterfly_data_preparation/download_rasters_from_planetary_computer.py)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47b21dbe88ae5caf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Filter satellite images and save final"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ebbd5691fd50e6a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def filter_satellite_images(dst, image_folder=\"raw_images\"):\n",
    "    \"\"\" excludes satellite images that are less than 128x128\n",
    "    Parameters:\n",
    "        dst: folder name to save final images        \n",
    "    \"\"\"\n",
    "    if not os.path.exists(dst):\n",
    "        os.makedirs(dst)\n",
    "    \n",
    "    # src\n",
    "    file_list = glob.glob(os.path.join(root_dir, dataset_tag, image_folder,  \"*\"))\n",
    "    for i, file_path in enumerate(file_list):\n",
    "        with rio.open(file_path) as f:\n",
    "            r = f.read(3)\n",
    "            g = f.read(2)\n",
    "            b = f.read(1)\n",
    "        composite = np.stack((r, g, b), axis=-1)\n",
    "        if composite.shape[0] >= 128 and composite.shape[1] >= 128:\n",
    "            shutil.copy(file_path, dst)\n",
    "\n",
    "def get_final_hotspots(dst):\n",
    "    \"\"\" returns list of final images after filtering \n",
    "    Parameters:\n",
    "        dst: folder name to save final images\n",
    "    Returns:\n",
    "        List of final hotspots\n",
    "    \"\"\"\n",
    "    final_hotspots = []\n",
    "    file_list = glob.glob(os.path.join(dst, \"*\"))\n",
    "    for i, file_path in enumerate(file_list):\n",
    "        final_hotspots.append(str(Path(file_path).name.split(\".\")[0][1:]))\n",
    "    \n",
    "    print(len(final_hotspots))\n",
    "    \n",
    "    return final_hotspots\n",
    "\n",
    "def explore_satellite_images(image_folder):\n",
    "    \"\"\" visualizes satellite images\n",
    "    Parameters:\n",
    "        image_folder: image_folder\n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    file_list = glob.glob(os.path.join(root_dir, dataset_tag, image_folder, \"*\"))\n",
    "\n",
    "    # Select 8 random files from the list\n",
    "    random_files = random.sample(file_list, 8)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "\n",
    "    for i, file_path in enumerate(random_files):\n",
    "        with rio.open(file_path) as f:\n",
    "            r = f.read(3)\n",
    "            g = f.read(2)\n",
    "            b = f.read(1)\n",
    "\n",
    "        # Create a composite image from RGB channels\n",
    "        composite = np.stack((r, g, b), axis=-1)\n",
    "        print(composite.shape)\n",
    "\n",
    "        # Clip and normalize the values\n",
    "        normalized_composite = np.clip((composite / 10000), 0, 1)\n",
    "\n",
    "        # Get the title from the file name\n",
    "        title = file_path.split(\"/\")[-1]\n",
    "\n",
    "        # Plot the image in the corresponding subplot\n",
    "        ax = axes[i // 4, i % 4]\n",
    "        ax.imshow(normalized_composite)\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "    # Adjust spacing and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "explore_satellite_images(image_folder=\"images\")\n",
    "\n",
    "# exclude images of smaller sizes (less than 128x128)\n",
    "dst = os.path.join(root_dir, dataset_tag, \"images\")\n",
    "\n",
    "filter_satellite_images(dst=dst, image_folder=\"raw_images\")\n",
    "final_hotspots = get_final_hotspots(dst)\n",
    "\n",
    "# exclude hotspots in butterfly dataframe that are not in final hotspots\n",
    "butterfly_df = pd.read_csv(os.path.join(root_dir, dataset_tag, \"butterfly_data_clustered.csv\"))\n",
    "butterfly_df = butterfly_df[butterfly_df['hotspot_id'].isin(final_hotspots)]\n",
    "butterfly_df.to_csv(os.path.join(root_dir, dataset_tag, \"butterfly_hotspots_final.csv\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3209cbf434b23b1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save Butterfly targets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba5b8a1f020303db"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def group_clusters(butterfly_df):\n",
    "    \"\"\" groups observations according to the center of clusters (hotspot_ID)\n",
    "    Parameters:\n",
    "        butterfly_df: Pandas DataFrame of butterfly observations\n",
    "    Returns: \n",
    "        grouped_butterfly_data: Pandas DataFrame of butterfly observations after grouping\n",
    "    \"\"\"\n",
    "    grouped_butterfly_data = butterfly_df.groupby(['hotspot_id'])\n",
    "        \n",
    "    group_sizes = grouped_butterfly_data.size()\n",
    "    \n",
    "    print(group_sizes)\n",
    "    \n",
    "    return grouped_butterfly_data\n",
    "\n",
    "butterfly_df = pd.read_csv(os.path.join(root_dir, dataset_tag, \"butterfly_hotspots_final.csv\"))\n",
    "grouped_butterfly_data = group_clusters(butterfly_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6affcc239ec69a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_species_list(df):\n",
    "    \"\"\" save species list of all unique species, with their frequency in raw observations\n",
    "    Parameters:\n",
    "        df: Pandas DataFrame of butterfly observations\n",
    "    Returns: \n",
    "        species_list: list of species names\n",
    "    \"\"\"\n",
    "    species_df = df['species'].value_counts()\n",
    "    \n",
    "    species_df = species_df.reset_index()\n",
    "    species_df.columns = ['species', 'frequency']\n",
    "    \n",
    "    species_df.to_csv(os.path.join(root_dir, dataset_tag, 'species/full_species_list.csv'), index=False)\n",
    "    \n",
    "    species_list = species_df[\"species\"].values.tolist()\n",
    "    print(len(species_list), species_list)\n",
    "    \n",
    "    return species_list\n",
    "\n",
    "butterfly_df = pd.read_csv(os.path.join(root_dir, dataset_tag, \"butterfly_hotspots_final.csv\"))\n",
    "species_list = save_species_list(butterfly_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8760fb51298a9ab0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Below is an intermediate step done only once: "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "467b70c3a23235f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# saving the list of species (common and union) between the hotspots here and ebird hotspots\n",
    "species_list_v1 = pd.read_csv(os.path.join(root_dir, dataset_tag, 'species/full_species_list.csv'))\n",
    "species_list_v2 = pd.read_csv(os.path.join(root_dir, \"SatButterfly_v2\", 'species/full_species_list.csv')) # species list of SatButterfly-v2\n",
    "\n",
    "# choose butterfly species with occurrences >= 100, in both dataset versions\n",
    "species_list_v1 = species_list_v1[species_list_v1[\"frequency\"] >= 100]\n",
    "species_list_v2 = species_list_v2[species_list_v2[\"frequency\"] >= 100]\n",
    "\n",
    "save_files = False\n",
    "if save_files:\n",
    "    # save species lists with occurrences >= 100, in both dataset versions\n",
    "    species_list_v1.to_csv(os.path.join(root_dir, dataset_tag, 'species/species_list_occurrences_ge100.csv'), index=False)\n",
    "    species_list_v2.to_csv(os.path.join(root_dir, \"SatButterfly_v2\", 'species/species_list_occurrences_ge100.csv'), index=False)\n",
    "\n",
    "# get intersection and union of species \n",
    "final_species_list_intersect = np.intersect1d(species_list_v1['species'].values.tolist(), species_list_v2['species'].values.tolist())\n",
    "final_species_list_union = np.union1d(species_list_v1['species'].values.tolist(), species_list_v2['species'].values.tolist())\n",
    "\n",
    "print(\"Final species list: \", final_species_list_intersect)\n",
    "\n",
    "if save_files:\n",
    "    # save lists of species (intersection and union)\n",
    "    with open(os.path.join(root_dir, \"species/intersection_species_list_occurrences_ge100.txt\"), 'w') as outfile:\n",
    "        outfile.write('\\n'.join(str(species_name) for species_name in final_species_list_intersect))\n",
    "    with open(os.path.join(root_dir, \"species/union_species_list_occurrences_ge100.txt\"), 'w') as outfile:\n",
    "        outfile.write('\\n'.join(str(species_name) for species_name in final_species_list_union))\n",
    "\n",
    "# save new species lists for SatButterfly_v1 with the intersection of species only\n",
    "new_species_list = species_list_v1[species_list_v1['species'].isin(final_species_list_intersect)]\n",
    "if save_files:\n",
    "    new_species_list.to_csv(os.path.join(root_dir, dataset_tag, 'species/species_list_updated_172species.csv'), index=False)\n",
    "\n",
    "# save new species lists for SatButterfly_v2 with the intersection of species only\n",
    "new_species_list_2 = species_list_v2[species_list_v2['species'].isin(final_species_list_intersect)]\n",
    "if save_files:\n",
    "    new_species_list_2.to_csv(os.path.join(root_dir, \"SatButterfly_v2\", 'species/species_list_updated_172species.csv'), index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7016b8268237ff5f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute ebutterfly target files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "354e5e80b7b12ab2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_target_files(grouped_butterfly_data, output_folder, species_list):\n",
    "    \"\"\"Creates targets by aggregating checklists, and saves json file per hotspot in output_folder\n",
    "    Parameters:\n",
    "        grouped_butterfly_data: Pandas DataFrame all butterfly observations, grouped per hotspot\n",
    "        output_folder: folder name to save .json target files\n",
    "        species_list: list of species to consider\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    for group_name, group_data in tqdm(grouped_butterfly_data):\n",
    "        target = {}\n",
    "        checklist_ = np.zeros(len(species_list))\n",
    "        # dropping species that appear more than once in the same checklist\n",
    "        new_df = group_data[['eventID', 'species']]\n",
    "        new_df = new_df.drop_duplicates(['eventID', 'species'])\n",
    "\n",
    "        for sp in new_df['species']:\n",
    "            if sp in species_list:\n",
    "                checklist_[species_list.index(sp)] += 1\n",
    "        target['num_complete_checklists'] = len(group_data['eventID'].unique())\n",
    "        # # NEW: to match SatBird scale\n",
    "        # if target['num_complete_checklists'] < 5:\n",
    "        #     continue\n",
    "        checklist_ = checklist_ / target['num_complete_checklists']\n",
    "        # if np.max(checklist_) > 1 :\n",
    "        #     problamtic.append(group_name)\n",
    "        \n",
    "        hs_name = \"B\" + group_name[0]\n",
    "        target['probs'] = checklist_.tolist()\n",
    "        target['hotspot_id'] = hs_name\n",
    "\n",
    "        with open(os.path.join(output_folder, hs_name + \".json\"), 'w') as fp:\n",
    "            json.dump(target, fp)\n",
    "\n",
    "dst = os.path.join(root_dir, dataset_tag, \"butterfly_targets_v1.2\")\n",
    "if not os.path.exists(dst):\n",
    "    os.makedirs(dst)\n",
    "\n",
    "final_species_list_intersect = open(os.path.join(root_dir, 'species/intersection_species_list_occurrences_ge100.txt')).read().split(\"\\n\")\n",
    "compute_target_files(grouped_butterfly_data, dst, final_species_list_intersect)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eecc919836c5f329"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### save final csv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54d2bb9b8a1ec333"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_final_csv(grouped_butterfly_data, output_filename='butterfly_hotspots.csv'):\n",
    "    \"\"\" saves csv file with final hotspots with columns: hotspot_name, lon, lat, number_of_observations, number_of_unique_checklists, number_of_unique_species, env variables\n",
    "    Parameters:\n",
    "        grouped_butterfly_data: Pandas DataFrame all butterfly observations, grouped per hotspot\n",
    "    \"\"\"\n",
    "    hotspot_ids = []\n",
    "    lats, lons = [], []\n",
    "    number_of_butterfly_obs = []\n",
    "    number_of_unique_checklists = []\n",
    "    number_of_different_species = []\n",
    "    states = []\n",
    "\n",
    "    for group_name, group_data in tqdm(grouped_butterfly_data):\n",
    "        # if group_name in final_hotspots:\n",
    "        # if len(group_data['eventID'].unique()) < 5:\n",
    "        #     continue\n",
    "        hotspot_ids.append(\"B\" + group_name[0])\n",
    "        lats.append(group_data['center_lat'].iloc[0])\n",
    "        lons.append(group_data['center_lon'].iloc[0])\n",
    "        states.append(group_data['stateProvince'].iloc[0])\n",
    "        number_of_butterfly_obs.append(len(group_data['occurrenceID']))\n",
    "        number_of_unique_checklists.append(len(group_data['eventID'].unique()))\n",
    "        number_of_different_species.append(len(group_data['species'].unique()))\n",
    "\n",
    "    final_data_frame = pd.DataFrame({'hotspot_id': hotspot_ids,\n",
    "                                     'lat': lats,\n",
    "                                     'lon': lons,\n",
    "                                     'stateProvince': states,\n",
    "                                     'ebutterfly_occurances': number_of_butterfly_obs,\n",
    "                                     'num_checklists': number_of_unique_checklists,\n",
    "                                     'num_species': number_of_different_species})\n",
    "\n",
    "    print(final_data_frame.head())\n",
    "\n",
    "    final_data_frame.to_csv(os.path.join(root_dir, dataset_tag, output_filename) , index=False)\n",
    "    \n",
    "save_final_csv(grouped_butterfly_data, 'butterfly_hotspots_all.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7eec04b99aaf5951"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract environmental rasters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d19b1991d78a3a61"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### use the polygons file to extract environmental rasters, using the script (data_processing/environmental/get_env_var.py)\n",
    "#### preferably on the cluster for the use of (geolifeclef-2022/rasters) data, using final csv file 'butterfly_hotspots_all.csv'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e88cd7a0726958"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dst = os.path.join(root_dir, dataset_tag, \"environmental_data\")\n",
    "if not os.path.exists(dst):\n",
    "    os.makedirs(dst)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2053ea1f8d3676f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### split data using DBSCAN (script: data_processing/utils/make_splits_by_distance.py), using the final csv (butterfly_hotspots_all.csv)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a61f63b53aaa058a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_different_split_files(butterfly_data_with_split):\n",
    "    \"\"\"\n",
    "    saves different csv files for each train/val/test split\n",
    "    \"\"\"\n",
    "    grouped_butterfly_data = butterfly_data_with_split.groupby(['split'], as_index=False)\n",
    "\n",
    "    for group_name, group_data in tqdm(grouped_butterfly_data):\n",
    "        print(group_name)\n",
    "        print(group_data[\"ebutterfly_occurances\"].max())\n",
    "        group_data.to_csv(os.path.join(root_dir, dataset_tag, \"butterfly_hotspots_\" + str(group_name[0]) + \".csv\"))\n",
    "        \n",
    "butterfly_data_with_split = pd.read_csv(os.path.join(root_dir, dataset_tag, \"butterfly_hotspots_with_splits.csv\"))\n",
    "\n",
    "save_different_split_files(butterfly_data_with_split)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5c8a0f451d7e288"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize map after splitting"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1fbedb12f290d57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot final splits\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "sys.path.append(str(Path().resolve().parent.parent))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "def plot_final_splits(df):\n",
    "    geoDatav = gpd.read_file('https://raw.githubusercontent.com/holtzy/The-Python-Graph-Gallery/master/static/data/US-counties.geojson')\n",
    "\n",
    "    geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry)   \n",
    "\n",
    "    ig, ax = plt.subplots(figsize =(15,10))\n",
    "    #train_gdf.drop_duplicates([\"geometry\"]).boundary.plot(ax = ax, alpha = 0.4, edgecolor = \"gray\")\n",
    "    geoDatav[~geoDatav[\"STATE\"].isin([\"02\", \"15\"])].boundary.plot(ax=ax, alpha = 0.1, edgecolor = \"gray\" )\n",
    "    gdf[gdf[\"split\"]==\"train\"].plot(ax=ax,marker='o', color='mediumslateblue', markersize=1, label = \"train\")\n",
    "    gdf[gdf[\"split\"]==\"valid\"].plot(ax=ax, marker='o', color='lightseagreen', markersize=1, label = \"valid\")\n",
    "    gdf[gdf[\"split\"]==\"test\"].plot(ax=ax, marker='o', color='lightsalmon', markersize=1, label = \"test\")\n",
    "\n",
    "    plt.legend(fontsize=16, markerscale=5,loc='lower right',  bbox_to_anchor=(0.92, 0.25))\n",
    "    plt.title(\"butterfly Hotspots\")\n",
    "    plt.show()\n",
    "    ig.savefig(os.path.join(root_dir, dataset_tag, \"satbutterfly_v1_data_dist.pdf\"),  bbox_inches='tight')\n",
    "\n",
    "    \n",
    "path = os.path.join(root_dir, dataset_tag, \"butterfly_hotspots_with_splits.csv\")\n",
    "plot_final_splits(df=pd.read_csv(path))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1078ec29fbfb3a12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Final files saved:\n",
    "[('valid', 1147), ('test', 1145), ('train', 5316)]\n",
    "- butterfly_hotspots_all.csv\n",
    "- butterfly_hotspots_train.csv\n",
    "- butterfly_hotspots_valid.csv\n",
    "- butterfly_hotspots_test.csv\n",
    "- species_list.csv\n",
    "- butterfly_targets/\n",
    "- environmental_data/\n",
    "- images\n",
    "- images_visual"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cf630c799e3e70b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
