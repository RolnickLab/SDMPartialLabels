{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad81cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path    \n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from geopy.distance import great_circle\n",
    "from shapely.geometry import MultiPoint\n",
    "from shapely.geometry import Polygon, Point\n",
    "from math import cos, radians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b676c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home/hagerradi/projects/Ecosystem_embeddings/ebutterfly/Darwin/0177350-230224095556074\"\n",
    "dataset_tag = \"ebutterfly_data_v4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57c179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_data_path = os.path.join(root_dir, dataset_tag, \"environmental_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cb9fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in glob.glob(env_data_path + '/*.npy'):\n",
    "    shutil.copy(file_name, env_data_path + '/B' + Path(file_name).name)\n",
    "\n",
    "for file_name in glob.glob(env_data_path + '/L*.npy'):\n",
    "    shutil.move(file_name, os.path.join(root_dir, dataset_tag, \"backup\", \"environmental_data\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f1bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data_path = os.path.join(root_dir, dataset_tag, \"images\")\n",
    "for file_name in glob.glob(img_data_path + '/*.tif'):\n",
    "    shutil.copy(file_name, img_data_path + '/B' + Path(file_name).name)\n",
    "\n",
    "for file_name in glob.glob(img_data_path + '/L*.tif'):\n",
    "    shutil.move(file_name, os.path.join(root_dir, dataset_tag, \"backup\", \"images\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7127da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_path = os.path.join(root_dir, dataset_tag, \"butterfly_targets\")\n",
    "for file_name in glob.glob(targets_path + '/*.json'):\n",
    "    shutil.copy(file_name, targets_path + '/B' + Path(file_name).name)\n",
    "\n",
    "for file_name in glob.glob(targets_path + '/L*.json'):\n",
    "    shutil.move(file_name, os.path.join(root_dir, dataset_tag, \"backup\", \"butterfly_targets\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6193eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.path.join(root_dir, dataset_tag, \"images_visual\")\n",
    "for file_name in glob.glob(img_path + '/*.tif'):\n",
    "    shutil.copy(file_name, img_path + '/B' + Path(file_name).name)\n",
    "\n",
    "for file_name in glob.glob(img_path + '/L*.tif'):\n",
    "    shutil.move(file_name, os.path.join(root_dir, dataset_tag, \"backup\", \"images_visual\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be963a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group_name in [\"train\", \"test\", \"valid\"]:\n",
    "    group_data = pd.read_csv(os.path.join(root_dir, dataset_tag, \"butterfly_hotspots_\" + str(group_name) + \".csv\"))\n",
    "    hotspots = group_data['hotspot_id'].values.tolist()\n",
    "    print(group_data)\n",
    "    new_hotspots = []\n",
    "    for hs in hotspots:\n",
    "        new_hotspots.append(\"B\"+hs)\n",
    "    \n",
    "    group_data['hotspot_id'] = new_hotspots\n",
    "    group_data.to_csv(os.path.join(root_dir, dataset_tag, \"butterfly_hotspots_\" + str(group_name) + \".csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d172ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group_name in [\"train\", \"test\", \"valid\"]:\n",
    "    group_data = pd.read_csv(os.path.join(root_dir, dataset_tag, \"butterfly_hotspots_\" + str(group_name) + \".csv\"))\n",
    "    print(group_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947159ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path    \n",
    "import shutil\n",
    "\n",
    "env_data_path = \"environmental_data\"\n",
    "for file_name in glob.glob(env_data_path + '/*.npy'):\n",
    "    shutil.move(file_name, env_data_path + '/B' + Path(file_name).name)\n",
    "    \n",
    "\n",
    "img_data_path = \"images\"\n",
    "for file_name in glob.glob(img_data_path + '/*.tif'):\n",
    "    shutil.move(file_name, img_data_path + '/B' + Path(file_name).name)\n",
    "\n",
    "targets_data_path = \"butterfly_targets\"\n",
    "for file_name in glob.glob(targets_data_path + '/*.json'):\n",
    "    shutil.move(file_name, targets_data_path + '/B' + Path(file_name).name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ba2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# targets_path = os.path.join(root_dir, dataset_tag, \"butterfly_targets\")\n",
    "targets_path = \"corrected_targets\"\n",
    "all_targets = []\n",
    "for file_name in tqdm(glob.glob(targets_path + '/*.json'):\n",
    "    targets = json.load(open(file_name))['probs']\n",
    "    targets = [i for i in targets if i != 0]\n",
    "    all_targets += targets\n",
    "\n",
    "print(len(all_targets))\n",
    "\n",
    "# all_targets = [i for i in all_targets if i != 0]\n",
    "# print(len([i for i in all_targets if i <= 1]))\n",
    "\n",
    "ret = plt.hist(all_targets, bins=5)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb34906",
   "metadata": {},
   "source": [
    "#### saving ebird targets with only most frequent top 28%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc011b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# N = int(0.25 * len(data))\n",
    "\n",
    "# # Get the indices of the top N values\n",
    "# top_indices = np.argsort(data)[-N:]\n",
    "\n",
    "# # Sort the top_indices in ascending order\n",
    "# sorted_top_indices = np.sort(top_indices)\n",
    "\n",
    "# targets_path = os.path.join(root_dir, dataset_tag, \"butterfly_targets\")\n",
    "targets_path = \"corrected_targets\"\n",
    "dst_path = \"corrected_targets_v2\"\n",
    "indices_to_save = np.load('stats/top_28percent_species_indices.npy')\n",
    "all_targets = []\n",
    "#print(indices_to_save)\n",
    "for file_name in tqdm(glob.glob(targets_path + '/*.json')):\n",
    "    targets = json.load(open(file_name))\n",
    "    #print(targets[\"probs\"])\n",
    "    new_probs = [targets[\"probs\"][i] for i in indices_to_save] \n",
    "    targets['probs'] = new_probs\n",
    "    #print(targets, len(new_probs))\n",
    "    with open(os.path.join(dst_path, os.path.basename(file_name)), 'w') as f:\n",
    "        json.dump(targets, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aee204",
   "metadata": {},
   "source": [
    "## Mapping family/order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91220102",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home/hagerradi/projects/Ecosystem_embeddings/ebutterfly/ebird_species\"\n",
    "species_df = pd.read_csv(os.path.join(root_dir, \"NEW_Clements-Checklist-v2022-October-2022.csv\"))\n",
    "\n",
    "species_list = open(os.path.join(root_dir, \"species_list_USA_birds.txt\")).read().split(\"\\n\")[0:-1]\n",
    "print(len(species_list))\n",
    "print(species_df.columns)\n",
    "\n",
    "species_family = []\n",
    "species_order = []\n",
    "\n",
    "for sp in species_list:\n",
    "    index_position = species_df[species_df['scientific name'] == sp].index[0]\n",
    "    species_family.append(species_df['family'][index_position])\n",
    "    species_order.append(species_df['order'][index_position])\n",
    "\n",
    "\n",
    "# f = open(os.path.join(root_dir, \"species_list_USA_birds_family.txt\"),'w')\n",
    "# f.write('\\n'.join(str(i) for i in species_family))\n",
    "# f.close()\n",
    "\n",
    "# f = open(os.path.join(root_dir, \"species_list_USA_birds_order.txt\"),'w')\n",
    "# f.write('\\n'.join(str(i) for i in species_order))\n",
    "# f.close()\n",
    "\n",
    "def save_mapping(species_l, file_name):\n",
    "    print(len(np.unique(species_l)))\n",
    "    unique_values , indices = np.unique(species_l, return_index=True)\n",
    "    species_l = np.array(species_l)\n",
    "    all_indices = {value: np.where(species_l == value)[0].tolist() for i, value in enumerate(unique_values)}\n",
    "    \n",
    "    all_v = []\n",
    "    for k, v in all_indices.items():\n",
    "        all_v += v\n",
    "\n",
    "    assert len(all_v) == len(species_list)\n",
    "\n",
    "    with open(os.path.join(root_dir, file_name), 'w') as f:\n",
    "        json.dump(all_indices, f)\n",
    "\n",
    "    \n",
    "    with open(os.path.join(root_dir, file_name), 'r') as f:\n",
    "        dict_loaded = json.load(f)\n",
    "    \n",
    "    print(len(dict_loaded.keys()))\n",
    "    print(list(dict_loaded.values())[0])\n",
    "\n",
    "save_mapping(species_l=species_order, file_name='bird_species_order_mapping.json')\n",
    "save_mapping(species_l=species_family, file_name='bird_species_family_mapping.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ec8d5f",
   "metadata": {},
   "source": [
    "## Analysis on preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e695d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import glob, os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "targets_path = \"/network/projects/ecosystem-embeddings/SatBird_data_v2/USA_summer/corrected_targets\"\n",
    "preds_path = \"/home/mila/h/hager.radi/scratch/ecosystem-embedding/baseline_joint_rtran_resnet18_RGBNIR_ENV_8/birds_preds\"\n",
    "\n",
    "for file_name in tqdm(glob.glob(os.path.join(preds_path, '*.npy'))):\n",
    "    hotspot_id = os.path.basename(file_name).split('.')[0]\n",
    "    pred = np.load(os.path.join(preds_path, hotspot_id + '.npy'))[0:670]\n",
    "    y = json.load(open(os.path.join(targets_path, hotspot_id + '.json')))\n",
    "    y = y['probs']\n",
    "    for y_, pred_ in zip(y, pred):\n",
    "        if y_ > 0:\n",
    "            print(y_, pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8555b27a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
